# Hybrid LLM Gateway ğŸš€

A production-ready hybrid AI gateway that intelligently routes chat requests between a **local LLM (Ollama)** and **DigitalOcean Gradientâ„¢ AI Agents**, with Redis-backed API key management and automatic routing based on intent and token size.

---

## âœ¨ Features

- ğŸ§  **Smart Routing**

  - Routes requests to **local Ollama** or **DO AI Agent**
  - Based on token count, intent detection, or explicit override

- ğŸ” **API Key Authentication**

  - API keys stored and validated via Redis
  - Rate limiting & daily usage limits supported

- âš¡ **Local + Cloud Hybrid**

  - Low-latency local inference
  - High-quality cloud inference for complex queries

- ğŸ§© **RAG-ready**

  - Supports DigitalOcean Agents with attached Knowledge Bases

- ğŸ³ **Dockerized**
  - One-command startup using Docker Compose

---

## ğŸ— Architecture

Client (curl / frontend)
|
v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hybrid LLM Gateway â”‚
â”‚ (Fastify + Node.js) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
|
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
| |
v v
Ollama (Local) DO AI Agent
(deepseek/qwen) (20B / RAG)

---

## ğŸ§° Tech Stack

- **Node.js 22**
- **Fastify**
- **Redis**
- **Ollama**
- **DigitalOcean Gradientâ„¢ AI Agents**
- **Docker & Docker Compose**

---

## ğŸš€ Quick Start (Local)

### 1ï¸âƒ£ Clone the repo

````bash
git clone https://github.com/your-username/hybrid-llm-gateway.git
cd hybrid-llm-gateway


---

### 2ï¸âƒ£ Create `.env`

```bash
cp .env.example .env
````

Fill in:

```env
PORT=3000

REDIS_HOST=redis
REDIS_PORT=6379

USE_DO_AGENT=true
LOCAL_MAX_TOKENS=512

DO_AGENT_ENDPOINT=https://<your-agent>.agents.do-ai.run
DO_AGENT_ACCESS_KEY=your-access-key
```

---

### 3ï¸âƒ£ Start services

```bash
docker compose up --build
```

---

## ğŸ§ª Test with curl

```bash
curl http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-api-key: dev-key-123" \
  -d '{
    "messages": [
      { "role": "user", "content": "Explain microservices architecture in detail" }
    ]
  }'
```

---

## ğŸ§  Routing Logic

Requests are routed to **DO Agent** when:

- Token count exceeds `LOCAL_MAX_TOKENS`
- Heavy intent is detected
- `modelPreference: "large"` is used

Otherwise, requests are served by **local Ollama**.

---

## ğŸ” API Key Management

- API keys are stored in Redis
- Seed keys using:

```bash
node src/scripts/sendAPIKeys.js
```

---

## ğŸ“¦ Deployment

This service is designed to be deployed on:

- DigitalOcean Droplets
- Any Docker-compatible VM

> Note: `.env` files are **not committed** to GitHub.

---

## ğŸ§¹ Cleanup

To fully stop and clean:

```bash
docker compose down -v
docker system prune -af
```

---

## ğŸ“œ License
